{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypBaFfRl8229"
      },
      "source": [
        "**CS 4650 \"Natural Language Processing\" - Project 1**  \n",
        "Georgia Tech, Spring 2023 \n",
        "\n",
        "(Instructor: Wei Xu; TAs: Mounica Maddela, Ben Podrazhansky, Marcus Ma, Rahul Katre)\n"
      ],
      "id": "ypBaFfRl8229"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "### Part 0. Google Colab Setup"
      ],
      "id": "JnnwupedFEIV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiFjTT4jFEIW"
      },
      "source": [
        "Welcome to the first full programming project for CS 4650! If you're new to Google Colab we recommend looking at [this](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) intro notebook before getting started with this project. In short, Colab is a Jupyter notebook environment that runs in the cloud, it's recommended for all of the programming projects in this course due to its availability, ease of use, and hardware accessibility. Some features that you may find especially useful are on the left hand side, these being:\n",
        "\n",
        "*   Table of contents: displays the sections of the notebook made using text cells\n",
        "*   Variables: useful for debugging and see current values of variables\n",
        "*   Files: useful or uploading or downloading any files you upload to Colab or write while working on the projects\n",
        "\n",
        "\n",
        "\n",
        "**To begin this project, make a copy of this notebook and save it to your local drive so that you can edit it.**\n",
        "\n",
        "\n",
        "If you want GPU's (which will improve training times), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "If you're new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2022/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch Basics notebook](http://bit.ly/pytorchbasics). Additionally, this [Text Sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch for NLP specific problems. "
      ],
      "id": "SiFjTT4jFEIW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "### Part 1. Loading and Preprocessing Data [10 points]\n",
        "The following cell loads the OnionOrNot dataset, and tokenizes each data item"
      ],
      "id": "ZRdPqeMMFEIY"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmV_uknBJA-o",
        "outputId": "f92babcf-46b8-4cc3-e349-583689934983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1903k  100 1903k    0     0  8689k      0 --:--:-- --:--:-- --:--:-- 8689k\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv > OnionOrNot.csv"
      ],
      "id": "YmV_uknBJA-o"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "L3DkMDu7FEIZ"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY #\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# this is how we select a GPU if it's avalible on your computer or in the Colab environment.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "L3DkMDu7FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "source": [
        "#### Part 1.1 Preprocessing definitions:\n",
        "The following cell define some methods to clean the dataset. Do not edit it, but feel free to take a look at some of the operations it's doing. \n"
      ],
      "id": "0Fulh0MZ8y8b"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "# example code taken from fast-bert\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "id": "ctNnE1Ui8oKw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "source": [
        "#### Part 1.2 Clean the data using the methods above and tokenize it using NLTK"
      ],
      "id": "MiUlTSBB9Wx6"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqtdrhF8FEIZ",
        "outputId": "bcac85b0-72f6-4f39-d7aa-0d9da5d01d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "df              = pd.read_csv(\"OnionOrNot.csv\")\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
      ],
      "id": "vqtdrhF8FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :). If you're unfamiliar with pandas, it's a extremely useful and popular library for data analysis and manipulation. You can find their documentation [here](https://pandas.pydata.org/docs/). \n",
        "\n",
        "Pandas primary data structure is a DataFrame. The following cell will print out the basic information of this structure, including the labeled axes (both columns and rows) as well as show you what the first n (default=5) rows look like"
      ],
      "id": "qBBdVOYxFEIa"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sJjScqV3FEIb",
        "outputId": "797505e4-d737-4142-ffaa-e441c1ae28ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
              "4  For men looking for great single women, online...      1   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
              "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
              "2  [bold, move, :, hulu, has, announced, that, th...  \n",
              "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
              "4  [for, men, looking, for, great, single, women,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8399d1f2-b097-44c2-bc41-c0ca17b71838\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8399d1f2-b097-44c2-bc41-c0ca17b71838')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8399d1f2-b097-44c2-bc41-c0ca17b71838 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8399d1f2-b097-44c2-bc41-c0ca17b71838');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "df.head()"
      ],
      "id": "sJjScqV3FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "source": [
        "DataFrames can be indexed using [.iloc\\[ ]](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html), this primarily uses interger based indexing and supports a single integer (i.e. 42), a list of integers (i.e. [1, 5, 42]), or even a slice (i.e. 7:42). "
      ],
      "id": "D9b4W9z1XhgS"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntm8laX6FEIb",
        "outputId": "65d3f804-d1dc-4455-fd39-9b89417bfd1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         Customers continued to wait at drive-thru even...\n",
              "label                                                        0\n",
              "tokenized    [customers, continued, to, wait, at, drive-thr...\n",
              "Name: 42, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "df.iloc[42]"
      ],
      "id": "Ntm8laX6FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "source": [
        "#### Part 1.3 Split the dataset into training, validation, and testing"
      ],
      "id": "TQVT6HUA9htQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "Now that we've loaded this dataset, we need to split the data into train, validation, and test sets. A good explanation of why we need these different sets can be found in subsection 2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but at the end it comes down to having a trustworthy and generalized model. The validation (sometimes called a development or tuning) set is used to help choose hyperparameters for our model, whereas the training set is used to fit the learned parameters (weights and biases) to the task. The test set is used to provide a final unbiased evaluation of our trained model, hopefully providing some insight into how it would actually do in production. Each of these sets should be disjoint from the others, to prevent any \"peeking\" that could unfairly influence our understanding of the model's accuracy. \n",
        "\n",
        "In addition to these different sets of data, we also need to create a vocab map for words in our Onion dataset, which will map tokens to numbers. This will be useful later, since torch PyTorch use tensors of sequences of numbers as inputs. **Go to the following cell, and fill out split_train_val_test and generate_vocab_map.**"
      ],
      "id": "GDI72x8XFEIc"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "outputs": [],
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "\n",
        "# split_train_val_test\n",
        "# This method takes a dataframe and splits it into train/val/test splits.\n",
        "# It uses the props argument to split the dataset appropriately.\n",
        "#\n",
        "# args:\n",
        "# df - the entire dataset DataFrame \n",
        "# props - proportions for each split in the order of [train, validation, test]. \n",
        "#         the last value of the props array is repetitive, but we've kept it for clarity.\n",
        "#\n",
        "# returns: \n",
        "# train DataFrame, val DataFrame, test DataFrame\n",
        "#\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "    \n",
        "    ## YOUR CODE STARTS HERE (~3-5 lines of code) ##\n",
        "    # hint: you can use df.iloc to slice into specific indexes or ranges.\n",
        "    length = df.shape[0]\n",
        "    train_df = df.iloc[:int(length * props[0])]\n",
        "    test_df = df.iloc[int(length * props[0]): int(length * (props[0] + props[1]))]\n",
        "    val_df = df.iloc[int(length * (props[0] + props[1])):]\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# generate_vocab_map\n",
        "# This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "# It uses the cutoff argument to remove rare words occuring <= cutoff times. \n",
        "# *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "# later. You'll also find the Counter imported for you to be useful as well.\n",
        "# \n",
        "# args:\n",
        "# df     - the entire dataset this mapping is built from \n",
        "# cutoff - we exclude words from the vocab that appear less than or\n",
        "#          eq to cutoff\n",
        "#\n",
        "# returns: \n",
        "# vocab - dict[str] = int\n",
        "#         In vocab, each str is a unique token, and each dict[str] is a \n",
        "#         unique integer ID. Only elements that appear > cutoff times appear\n",
        "#         in vocab.\n",
        "#\n",
        "# reversed_vocab - dict[int] = str\n",
        "#                  A reversed version of vocab, which allows us to retrieve \n",
        "#                  words given their unique integer ID. This map will \n",
        "#                  allow us to \"decode\" integer sequences we'll encode using\n",
        "#                  vocab!\n",
        "# \n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = None\n",
        "    \n",
        "    ## YOUR CODE STARTS HERE (~5-15 lines of code) ##\n",
        "    # hint: start by iterating over df[\"tokenized\"]\n",
        "    count = 0\n",
        "    d = {}\n",
        "    uid = 1\n",
        "    for r in df[\"tokenized\"]:\n",
        "      for t in r:\n",
        "        if str(t) in d.keys():\n",
        "          d[str(t)] += 1\n",
        "        else:\n",
        "          d[str(t)] = 1\n",
        "        if d.get(str(t)) > cutoff and str(t) not in vocab.keys():\n",
        "          vocab[str(t)] = uid+1\n",
        "          uid += 1\n",
        "    count = 0\n",
        "    reversed_vocab = {}\n",
        "    for v in vocab.keys():\n",
        "      reversed_vocab[str(count)] = v\n",
        "      count += 1\n",
        "\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    return vocab, reversed_vocab"
      ],
      "id": "zeo9kX6i9pbH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LEk83hRFgT"
      },
      "source": [
        "With the methods you have implemented above, we can now split the dataset into training, validation, and testing sets and generate our dictionaries mapping from word tokens to IDs (and vice versa). \n",
        "\n",
        "Note: The props list currently being used splits the dataset so that 80% of samples are used to train, and the remaining 20% are evenly split between training and validation. How you split your dataset is itself a major choice and something you would need to consider in your own projects. Can you think of why?"
      ],
      "id": "w9LEk83hRFgT"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
      ],
      "id": "rcmX931OFEId"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAACzA8YFEId",
        "outputId": "d7bb7546-12c5-4764-e256-9629d1411068"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# This line of code will help test your implementation, the expected output is the same distribution used in 'props'\n",
        "#   in the above cell. Try out some different values to ensure it works, but for submission ensure you use \n",
        "#   [.8, .1, .1] \n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ],
      "id": "CAACzA8YFEId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "source": [
        "#### Part 1.4 Building a Dataset Class"
      ],
      "id": "5fCFfEHv1hnI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the HeadlineDataset class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \n",
        "for help."
      ],
      "id": "8-qTQQa2FEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "outputs": [],
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from torch.utils.data import Dataset\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "# HeadlineDataset\n",
        "# This class takes a Pandas DataFrame and wraps in a Torch Dataset.\n",
        "# Read more about Torch Datasets here: \n",
        "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "# \n",
        "class HeadlineDataset(Dataset):\n",
        "    \n",
        "    # initialize this class with appropriate instance variables\n",
        "    def __init__(self, vocab, df, max_length=50):\n",
        "        # For this method: We would *strongly* recommend storing the dataframe \n",
        "        #                  itself as an instance variable, and keeping this method\n",
        "        #                  very simple. Leave processing to __getitem__. \n",
        "        #              \n",
        "        #                  Sometimes, however, it does make sense to preprocess in \n",
        "        #                  __init__. If you are curious as to why, read the aside at the \n",
        "        #                  bottom of this cell.\n",
        "        # \n",
        "        \n",
        "        ## YOUR CODE STARTS HERE (~3 lines of code) ##\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        return \n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    # return the length of the dataframe instance variable\n",
        "    def __len__(self):\n",
        "\n",
        "        df_len = None\n",
        "        ## YOUR CODE STARTS HERE (1 line of code) ##\n",
        "        df_len = len(self.df)\n",
        "\n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        return df_len\n",
        "\n",
        "    # __getitem__\n",
        "    # \n",
        "    # Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    # using our vocab map created using generate_vocab_map. Restricts the encoded \n",
        "    # headline length to max_length.\n",
        "    # \n",
        "    # The purpose of this method is to convert the row - a list of words - into\n",
        "    # a corresponding list of numbers.\n",
        "    #\n",
        "    # i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    # this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "    #\n",
        "    # returns: \n",
        "    # tokenized_word_tensor - torch.LongTensor \n",
        "    #                         A 1D tensor of type Long, that has each\n",
        "    #                         token in the dataframe mapped to a number.\n",
        "    #                         These numbers are retrieved from the vocab_map\n",
        "    #                         we created in generate_vocab_map. \n",
        "    # \n",
        "    #                         **IMPORTANT**: if we filtered out the word \n",
        "    #                         because it's infrequent (and it doesn't exist \n",
        "    #                         in the vocab) we need to replace it w/ the UNK \n",
        "    #                         token\n",
        "    # \n",
        "    # curr_label            - int\n",
        "    #                         Binary 0/1 label retrieved from the DataFrame.\n",
        "    # \n",
        "    def __getitem__(self, index: int):\n",
        "        tokenized_word_tensor = None\n",
        "        curr_label            = None\n",
        "        ## YOUR CODE STARTS HERE (~3-7 lines of code) ##\n",
        "        tokenized_word_tensor = []\n",
        "        row = self.df.iloc[index]\n",
        "        for t in row[\"tokenized\"]:\n",
        "          if str(t) in self.vocab:\n",
        "            tokenized_word_tensor.append(self.vocab[str(t)])\n",
        "          else:\n",
        "            tokenized_word_tensor.append(self.vocab[\"UNK\"])\n",
        "        tokenized_word_tensor = torch.tensor(tokenized_word_tensor).long().to(device)\n",
        "        curr_label = row[\"label\"]\n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "# \n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data \n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "# \n",
        "# There is a tradeoff though: can you think of one?\n",
        "# "
      ],
      "id": "tqt9q92J1QKK"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers, they'll\n",
        "#   define how our DataLoaders sample elements from the HeadlineDatasets  \n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ],
      "id": "KuLtIOAZFEIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "source": [
        "#### Part 1.5 Finalizing our DataLoader"
      ],
      "id": "n9iBiSKF1yXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch DataLoaders to batch our data for us. **In the following cell fill out collate_fn.** Refer to PyTorch documentation on [DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ],
      "id": "lfXSbxoFFEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "outputs": [],
      "source": [
        "from tkinter.constants import Y\n",
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "# collate_fn\n",
        "# This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "# batched rows, in the form of tuples, from a DataLoader and applies some final \n",
        "# pre-processing.\n",
        "#\n",
        "# Objective:\n",
        "# In our case, we need to take the batched input array of 1D tokenized_word_tensors, \n",
        "# and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors \n",
        "# in a batch. We're moving from a Python array of tuples, to a padded 2D tensor. \n",
        "#\n",
        "# *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "# \n",
        "# Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "#\n",
        "# args: \n",
        "# batch - PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "#         len(batch) == BATCH_SIZE\n",
        "# \n",
        "# returns:\n",
        "# padded_tokens - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "# y_labels      - 1D FloatTensor of shape (BATCH_SIZE)\n",
        "# \n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "    padded_tokens, y_labels = None, None\n",
        "    ## YOUR CODE STARTS HERE (~4-8 lines of code) ##\n",
        "    pt = []\n",
        "    y = []\n",
        "    for b in batch:\n",
        "      pt.append(b[0])\n",
        "      y.append(b[1])\n",
        "    padded_tokens = torch.nn.utils.rnn.pad_sequence(pt, True, padding_value).long()\n",
        "    y_labels = torch.tensor(y, dtype = torch.float).to(device)\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return padded_tokens, y_labels"
      ],
      "id": "Zp1aQAvn1_mz"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "OayoJRTeFEIf"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pidbg12AFEIf",
        "outputId": "ba025399-3bc4-4f4b-da76-8cc12aa90e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  26,   24,  169,    1, 6120,   20,    1, 3102,    1,   11,    1,   43,\n",
            "          147,   10, 5724,   11,  140,   11, 5945,    1,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 177,    6,  394, 8222, 1213,  125, 7709, 2927,  596,  174,    2,  379,\n",
            "           22,    1,  269,  107,    7, 1168,   11,   73,    9,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [1053,    6,  284,   10,    1,    6,   36,   44, 1400,   54,   22, 3510,\n",
            "          663,    9, 1053,   11,   60,  931, 1331,    3,  186,   10,  970,    7,\n",
            "          443,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 450,  522,  120,   24, 7164,   44,  494,   89,  609,   10, 1447, 1188,\n",
            "           13,   10, 4987,    1, 2693,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [  77,  285, 1939,   71,    1,   32,  685,   11,   60,    1,    3, 4487,\n",
            "          179, 2741,   60,  572,  134,    1,    7, 2619,    9, 2620, 3830, 2135,\n",
            "           60,  198,  224, 1868,  106,    0,    0,    0],\n",
            "        [ 142,    6, 2185, 1422,    9,   23,   83, 1373,  235,  336,    1, 3951,\n",
            "         3294,    7, 7754,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 871,  925,  172,    2,    1,  570,   32,  871,  925,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [  83,    6, 2181,    6,   54, 4282, 1319,    1,  700, 6000,    9,    1,\n",
            "          187,   49,    9,   23,  270,  819,   32,   38,  120,   38, 2148,  219,\n",
            "          382,    3, 6756,    0,    0,    0,    0,    0],\n",
            "        [1618, 1243,   90, 3482,    3, 3728, 7430,   39, 5431,  658,  350,   14,\n",
            "           18, 2662,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 400,  786,  946, 1415,    2, 1836, 1177,    3,   42, 6016,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 912, 1152, 2017,  272,    3,  855, 1198,  500,    1, 1309,    6, 1762,\n",
            "         3028,  780,    9,   97,  755,    4,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [  83,    6, 2024,   15, 3020,    6,   54,   99,  797,    1,  102,   27,\n",
            "          116,   10,  101,    3,  945,  569,   12,   18,  664,   15,   33,  163,\n",
            "          270,  819,   38,    9,   23,   59, 1575, 3809],\n",
            "        [5313,  381,  477,  226,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [  47,    6, 3884,  391,   82, 1509,   43,  133,  224,    1, 5868,    1,\n",
            "          494,   89,   10,    1,   16,  145,    1,   89,   24, 1974,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 177,    6,   38,   84,  649, 1073,    7, 1583, 3516,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 296,   37,  153,    3,  674, 2819, 8884,   32,  114,  655,  107,  127,\n",
            "            2, 1603, 1712,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0') tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.],\n",
            "       device='cuda:0')\n",
            "x: torch.Size([16, 32])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# Use this to test your collate_fn implementation.\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn while running this snippet\n",
        "\n",
        "for x, y in test_iterator:\n",
        "    print(x, y)\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {y.shape}')\n",
        "    break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "pidbg12AFEIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "### Part 2: Modeling [10 pts]\n",
        "Let's move to modeling, now that we have dataset iterators that batch our data for us. In the following code block, you'll build a feed-forward neural network implementing a neural bag-of-words baseline, NBOW-RAND, described in section 2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You'll find [this](https://pytorch.org/docs/stable/nn.html) page useful for understanding the different layers and [this](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) page useful for how to put them into action.\n",
        "\n",
        "The core idea behind this baseline is that after we embed each word for a document, we average the embeddings to produce a single vector that hopefully captures some general information spread across the sequence of embeddings. This means we first turn each document of length *n* into a matrix of *nxd*, where *d* is the dimension of the embedding. Then we average this matrix to produce a vector of length *d*, summarizing the contents of the document and proceed with the rest of the network. \n",
        "\n",
        "While you're working through this implementation, keep in mind how the dimensions change and what each axes represents, as documents will be passed in as minibatches requiring careful selection of which axes you apply certain operations too. You're more than welcome to experiment with the architecture of this network as well outside of the basic setup we describe below, such as adding in other layers, to see how this changes your results."
      ],
      "id": "BWLK7T1uFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "source": [
        "#### Part 2.1 Define the NBOW model class"
      ],
      "id": "pZDPs0Sf-H3V"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "outputs": [],
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "import torch.nn as nn\n",
        "# END - DO NOT CHANGE THESE IMPORTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "    # Instantiate layers for your model-\n",
        "    # \n",
        "    # Your model architecture will be a feed-forward neural network.\n",
        "    #\n",
        "    # You'll need 3 nn.Modules at minimum\n",
        "    # 1. An embeddings layer (see nn.Embedding)\n",
        "    # 2. A linear layer (see nn.Linear)\n",
        "    # 3. A sigmoid output (see nn.Sigmoid)\n",
        "    #\n",
        "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    # \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE STARTS HERE (~4 lines of code) ##\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear_layer = nn.Linear(embedding_dim, 1)\n",
        "        self.output = nn.Sigmoid()\n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        \n",
        "    # Complete the forward pass of the model.\n",
        "    #\n",
        "    # Use the output of the embedding layer to create\n",
        "    # the average vector, which will be input into the \n",
        "    # linear layer.\n",
        "    # \n",
        "    # args:\n",
        "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    #     This is the same output that comes out of the collate_fn function you completed\n",
        "    #\n",
        "    # returns:\n",
        "    # 1D FloatTensor of shape (BATCH_SIZE)\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE STARTS HERE (~4-5 lines of code) ##\n",
        "        embd = self.embedding_layer(x)\n",
        "        lin = self.linear_layer(torch.mean(embd, dim = 1))\n",
        "        out = torch.flatten(self.output(lin))\n",
        "\n",
        "        return out\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    "
      ],
      "id": "jzGx2q0jLqyU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "source": [
        "#### Part 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, assuming you haven't added any additional layers, there's really only one hyperparameter for the model architecture: the size of the embedding dimension. \n",
        "\n",
        "The vocab_size parameter here is based on the number of unique words kept in the vocab after removing those occurring too infrequently, so this is determined by our dataset and is in turn not a true hyperparameter (though the cutoff we used previously might be). The embedding_dim parameter dictates what size vector each word can be embedded as. \n",
        "\n",
        "If you added additional linear layers to the NBOW model then the input/output dimensions of each would be considered a hyperparameter you might want to experiment with. While the sizes are constrained based on previous & following layers (the number of dimensions need to match for the matrix multiplication), whatever sequence you used could still be tweaked in various ways. \n",
        "\n",
        "A special note concerning the model initialization: We're specifically sending the model to the device set in Part 1, to speed up training if the GPU is available. **Be aware**, you'll have to ensure other tensors are on the same device inside your training and validation loops. "
      ],
      "id": "xltosIzM-SP2"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "model = NBOW(vocab_size    = len(train_vocab.keys()),\n",
        "             embedding_dim = 300).to(device)"
      ],
      "id": "_HQWUu-ZFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "source": [
        "#### Part 2.3 Instantiate the loss function and optimizer"
      ],
      "id": "C4CZnj1f-da-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "In the following cell, **select and instantiate an appropriate loss function and optimizer.** \n",
        "\n",
        "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at [PyTorch docs](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!"
      ],
      "id": "9aXi8nA0FEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "#while Adam is already imported, you can try other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "criterion, optimizer = None, None\n",
        "### YOUR CODE GOES HERE ###\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters())\n",
        "\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "id": "w98UvlXxFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "source": [
        "At this point, we have a NBOW model to classify headlines as being real or fake and a loss function/optimizer to train the model using the training dataset."
      ],
      "id": "hUXBtqPEjiRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "### Part 3: Training and Evaluation [10 Points]\n",
        "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below. Treat real headlines as False, and Onion headlines as True.**  Feel free to look at [PyTorch docs](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html) for help!"
      ],
      "id": "bVLeTa8wFEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "# returns the total loss calculated from criterion\n",
        "def train_loop(model, criterion, optim, iterator):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(iterator):\n",
        "        ### YOUR CODE STARTS HERE (~6 lines of code) ###\n",
        "        optim.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.item()\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "    return total_loss\n",
        "\n",
        "# returns:\n",
        "# - true: a Python boolean array of all the ground truth values \n",
        "#         taken from the dataset iterator\n",
        "# - pred: a Python boolean array of all model predictions. \n",
        "def val_loop(model, iterator):\n",
        "    true, pred = [], []\n",
        "    ### YOUR CODE STARTS HERE (~8 lines of code) ###\n",
        "    model.eval()\n",
        "    for x, y in tqdm(iterator):\n",
        "      p = torch.round(model(x))\n",
        "      for i in range(len(y)):\n",
        "        true.append(bool(p[i]))\n",
        "        pred.append(bool(y[i]))\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "    return true, pred"
      ],
      "id": "vganx5fCFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "source": [
        "#### Part 3.1 Define the evaluation metrics"
      ],
      "id": "JNXJevTu-tDZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We also need evaluation metrics that tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. **Complete the functions in the following cell.** You'll find subsection 4.4.1 of Eisenstein useful for this task."
      ],
      "id": "7IsZQs3rFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "outputs": [],
      "source": [
        "# DO NOT IMPORT ANYTHING IN THIS CELL. You shouldn't need any external libraries.\n",
        "\n",
        "# accuracy\n",
        "#\n",
        "# What fraction of classifications are correct?\n",
        "# \n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "# return: accuracy bounded between [0, 1]\n",
        "#\n",
        "def accuracy(true, pred):\n",
        "    acc = None\n",
        "    ## YOUR CODE STARTS HERE (~2-5 lines of code) ##\n",
        "\n",
        "    acc_list = torch.tensor([1 if pred[i] == true[i] else 0 for i in range(len(true))])\n",
        "    acc = torch.sum(acc_list)/ acc_list.size(dim=0)\n",
        "    \n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return acc\n",
        "\n",
        "# binary_f1 \n",
        "#\n",
        "# A method to calculate F-1 scores for a binary classification task.\n",
        "# \n",
        "# args -\n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "# selected_class: Boolean - the selected class the F-1 \n",
        "#                 is being calculated for.\n",
        "# \n",
        "# return: F-1 score between [0, 1]\n",
        "#\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "    f1 = None\n",
        "    ## YOUR CODE STARTS HERE (~10-15 lines of code) ##\n",
        "    #only true[i] == selected_class\n",
        "    true_positive_list = [1 if true[i] == selected_class and pred[i]  == selected_class else 0 for i in range(len(true))]\n",
        "    false_negative_list = [1 if true[i] == selected_class and not(pred[i] == selected_class) else 0 for i in range(len(true))]\n",
        "    true_negative_list = [1 if true[i] != selected_class and pred[i] != selected_class else 0 for i in range(len(true))]\n",
        "    false_positive_list = [1 if true[i] != selected_class and pred[i] == selected_class else 0 for i in range(len(true))]\n",
        "    tp = sum(true_positive_list)\n",
        "    fn = sum(false_negative_list)\n",
        "    tn = sum(true_negative_list)\n",
        "    fp = sum(false_positive_list)\n",
        "    if tp+fp == 0:\n",
        "      precision = 0\n",
        "    else:\n",
        "      precision = tp/(tp+fp)\n",
        "    if tp+fn == 0:\n",
        "      recall = 0\n",
        "    else:\n",
        "      recall = tp/(tp+fn)\n",
        "    if (precision + recall) == 0:\n",
        "      f1 = 0\n",
        "    else:\n",
        "      f1 = (2*precision*recall)/(precision + recall)\n",
        "    \n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return f1\n",
        "\n",
        "# binary_macro_f1\n",
        "# \n",
        "# Averaged F-1 for all selected (true/false) classes.\n",
        "#\n",
        "# args -\n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "#\n",
        "# return: F-1 score between [0, 1]\n",
        "#\n",
        "def binary_macro_f1(true, pred):\n",
        "    averaged_macro_f1 = None\n",
        "    ## YOUR CODE STARTS HERE (1 line of code) ##\n",
        "    averaged_macro_f1 = (binary_f1(true, pred, True) + binary_f1(true, pred, False))/2\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return averaged_macro_f1"
      ],
      "id": "gMQDg9Vy-wY0"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw79JFieFEIi",
        "outputId": "889ec96f-5a9a-4e62-aaa7-e8e30200c158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 253.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Binary Macro F1: 0.46096857472018293\n",
            "Accuracy: 0.4658333361148834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
        "# It should do pretty poorly, but this can be random because of the initialization of the parameters of the model.\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print()\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "Yw79JFieFEIi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BerBx-T3kZtC"
      },
      "source": [
        "At this point, we have our datasets defined and split, our model and training tools/loops, and evaluation metrics so we can finally move on to train our model and see how it does!"
      ],
      "id": "BerBx-T3kZtC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "### Part 4: Actually training the model [1 point]\n",
        "Watch your model train :D You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "Q2to0kWVFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-iuqkKCFEIj",
        "outputId": "2cf5d40d-0a1a-4929-c77c-0181ee4d02b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 187.86it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 250.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 620.3446144610643\n",
            "VAL F-1: 0.7739599559468822\n",
            "VAL ACC: 0.8029166460037231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 190.66it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 244.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 402.4617502242327\n",
            "VAL F-1: 0.8337455027605079\n",
            "VAL ACC: 0.8487499952316284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 187.83it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 254.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 315.853447265923\n",
            "VAL F-1: 0.8488844230852726\n",
            "VAL ACC: 0.8604166507720947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 188.00it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 252.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 266.03455318696797\n",
            "VAL F-1: 0.8530235317294305\n",
            "VAL ACC: 0.8616666793823242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 191.79it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 250.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 228.3734827330336\n",
            "VAL F-1: 0.8601146730890372\n",
            "VAL ACC: 0.8700000047683716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 187.87it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 248.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 201.04515806492418\n",
            "VAL F-1: 0.859845725481799\n",
            "VAL ACC: 0.8700000047683716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 6\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "N-iuqkKCFEIj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
      ],
      "id": "_l91F4ooFEIj"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs8Fy_ncFEIo",
        "outputId": "d6e3a2d4-cdac-49d0-ff0b-e22157d5e0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 255.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST F-1: 0.8663618260933696\n",
            "TEST ACC: 0.8741666674613953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print()\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "vs8Fy_ncFEIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "### Part 5: Analysis [5 points]\n",
        "Answer the following questions:\n",
        "\n"
      ],
      "id": "rMPWmorEFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "source": [
        "#### 1. What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?\n",
        "\n",
        "Answer: \n",
        "#####Zipf's Law states that the frequencies of an event are inversely proportional to rank r. In this case, we can consider the cutoff as the ranking, and the number of vocabulary that appears more than the \"cutoff\" value is related to the frequency. When cutoff = 0, the vocab_size of the whole vocabulary without cutoff is 25388. When cutoff = 1, vocab_size = 13298. When cutoff = 2, vocab_size = 9540. When cutoff = 3, vocab_size = 7612. We can see the cutoff is inversely proportional to the vocab_size. As the cutoff increase, the vocab_size is smaller."
      ],
      "id": "fnjKlKt352hQ"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI0fM4oMFEIp",
        "outputId": "055b2baf-5598-44d5-d19c-c7a89f269a2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7612"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 3)\n",
        "len(tmp_vocab)\n",
        "# tmp_vocab"
      ],
      "id": "pI0fM4oMFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "#### 2. Can you describe what cases the model is getting wrong in the witheld test-set? \n",
        "\n",
        "Answer:\n",
        "##### When the predicted label differs from the true label, the sequences are incorrectly predicted cases in the test set. We can observe that nearly all incorrect sequences have padding. And, 'UNK' appears very often in incorrect sequences. Because padding is adding '' to make the list of the max_length, adding elements that are not in the original list may cause the label to be predicted incorrectly. The 'UNK' are words that are not in the vocabulary. So the original word is undetected in the vocabulary, possibly making the label incorrect. The cases that the model is getting wrong more often in the test set I observed are having padding and 'UNK' in the sequences.\n"
      ],
      "id": "d0x54B1lFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yiIZov-583w"
      },
      "source": [
        "To do this, you'll need to create a new val_train_loop (``val_train_loop_incorrect``) so it returns incorrect sequences **and** you'll need to decode these sequences back into words. \n",
        "Thankfully, you've already created a map that can convert encoded sequences back to regular English: you will find the ``reverse_vocab`` variable useful.\n",
        "\n",
        "```\n",
        "# i.e. using a reversed map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "# we can turn [2, 3, 0] into this => [\"hi\", \"hello\", \"UNK\"]\n",
        "```"
      ],
      "id": "_yiIZov-583w"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "# Implement this however you like! It should look very similar to val_loop.\n",
        "# Pass the test_iterator through this function to look at errors in the test set.\n",
        "def val_train_loop_incorrect(model, iterator):\n",
        "  model.train()\n",
        "  incorrect_seq = [] \n",
        "  for x, y in tqdm(iterator):\n",
        "    p = torch.round(model(x))\n",
        "    for i in range(len(y)):\n",
        "      if p[i] != y[i]:\n",
        "        seq = \"\"\n",
        "        for j in x[i]:\n",
        "          seq += \" \" + reverse_vocab[str(j.item())]\n",
        "        incorrect_seq.append(seq)\n",
        "\n",
        "  return incorrect_seq"
      ],
      "id": "TfohtPF8FEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-azPje88iU0",
        "outputId": "e17e4056-2230-40bb-a8ff-11f720f621ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 206.89it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' anti-vaccine course brings u of t one step closer to offering a masters of UNK                ',\n",
              " ' saudi arabia officially lifts ban on female monster truck rallies                     ',\n",
              " ' why does it seem like movie UNK are harder on UNK UNK sex than UNK UNK violence ?       ',\n",
              " ' dallas cops plant black suspect at murder scene                 ',\n",
              " ' the nra is claiming that the tennessee waffle house shooting would not have happened if an armed teacher had been teaching a math class there',\n",
              " ' glenn beck thinks watch dogs teaches you how to hack the real world            ',\n",
              " ' UNK had to lie about her age to join facebook         ',\n",
              " \" man worried about drug dealer who 's not picking up phone        \",\n",
              " ' ‘ they can ’ t arrest both of us , ’ says giuliani UNK himself to times square pikachu',\n",
              " ' new law gives french workers ‘ right to UNK ’         ',\n",
              " ' ron paul will make his acting UNK in UNK UNK : part UNK        ',\n",
              " ' 2016 in technology                       ',\n",
              " ' breaking : there are 70,000 pounds of little UNK UNK on the UNK             ',\n",
              " ' vanilla ice decries lack of emotional truth in new ninja turtles rap              ',\n",
              " ' nra says parkland students should be grateful for guns giving them such a UNK UNK experience  ',\n",
              " ' beauty UNK forces college girls to twerk for UNK internship                   ',\n",
              " ' ‘ the bachelor ’ accused of UNK his power as a reality tv star to lure 30 women to california mansion        ',\n",
              " ' gay male escorts were the biggest winners at this week ’ s republican convention               ',\n",
              " ' UNK UNK opens museum dedicated to UNK UNK               ',\n",
              " ' UNK stars to return for UNK 2 , UNK UNK of UNK           ',\n",
              " ' a real shame : brian williams is being forced to resign from msnbc after leaked documents have revealed that he ’ s a UNK  ',\n",
              " ' election day by the numbers                     ',\n",
              " \" amish give up `` this is bullshit , '' UNK say               \",\n",
              " \" factory farm chicken rounds out miserable existence by going bad in man 's UNK            \",\n",
              " ' bill cosby feeling UNK after jury slips conviction into his verdict               ',\n",
              " ' federal officials investigating man posting racist attacks online armed with millions of explosives          ',\n",
              " ' miss america called before u.n. council for not promoting enough world peace         ',\n",
              " ' new discovery channel chief promises no more UNK bullshit            ',\n",
              " ' house conservatives introduce resolution to UNK UNK UNK             ',\n",
              " ' saudi prince visits injured UNK child in hospital to finish the job         ',\n",
              " ' man UNK scolds other man online for having opinion he held less than 2 years ago      ',\n",
              " ' glenn beck vows to UNK destroy ‘ UNK ’            ',\n",
              " ' UNK battle furiously over jennifer UNK               ',\n",
              " ' catholic church releases new UNK UNK boy UNK                    ',\n",
              " ' why UNK join isis                        ',\n",
              " ' UNK UNK blamed in massive model train crash                    ',\n",
              " \" fcc assures nation their favorite verizon websites wo n't be UNK by net neutrality repeal             \",\n",
              " ' a great grandmother has been praying to a lord of the rings figure thinking it was saint anthony       ',\n",
              " ' iowa residents UNK after strange sign bearing word ‘ kasich ’ appears on UNK overnight          ',\n",
              " ' parent : my son was given UNK by fellow first grader          ',\n",
              " ' kkk member indicted for trying to build UNK x-ray cannon                ',\n",
              " ' r. kelly releases emotional new song UNK fans for UNK acceptance of sex crimes                ',\n",
              " ' scientists propose UNK arctic ice                         ',\n",
              " ' american is fighter : i made a bad decision           ',\n",
              " ' you ’ re 8 years old . can you UNK wine without getting arrested ?     ',\n",
              " ' pilots who fly drones into UNK are idiots . punish them                  ',\n",
              " ' UNK                   ',\n",
              " ' amputee inspires others not to lose UNK             ',\n",
              " ' man arriving late forced to use excuse he was saving for leaving early      ',\n",
              " ' phoenix leads nation in UNK UNK lasers at UNK          ',\n",
              " ' 24 million would lose coverage under gop health plan          ',\n",
              " ' george r.r . martin promises fans ‘ the winds of winter ’ is nearly started    ',\n",
              " ' news : a UNK plea : UNK is urging people writing erotic ninja turtle fan fiction to keep in mind the physical challenges UNK by the turtles ’ UNK UNK during UNK',\n",
              " ' seasons turn UNK from the one that kills old people to the one that kills homeless people               ',\n",
              " ' ‘ UNK UNK led me here , ’ thinks naked woman sitting on public toilet with UNK around her UNK   ',\n",
              " ' report : some patients treated for putting UNK in their eyes during eclipse          ',\n",
              " \" in the know : are tests UNK against students who do n't give a shit ?       \",\n",
              " ' joe UNK so sick of fans asking to UNK UNK for photo UNK          ',\n",
              " \" god angrily clarifies 'do n't kill ' rule                \",\n",
              " \" report : a lot of people 's dream is to have sex with a ghost         \",\n",
              " ' former lovers meet in coffee shop for one last UNK                       ',\n",
              " ' weed delivery guy saves christmas                            ',\n",
              " ' indonesian mother UNK halloween costumes for 60,000 children                         ',\n",
              " ' starbucks to close 8,000 stores for racial bias training                        ',\n",
              " ' amazing UNK : america ’ s UNK UNK men wearing basketball shorts as bathing suits have UNK their annual migration to the jersey shore ’ s UNK punching bag UNK games ',\n",
              " \" bird hunted to near extinction due to infuriating UNK you ' call                    \",\n",
              " ' dad suggests arriving at airport 14 hours early            ',\n",
              " ' ask a college professor having trouble with the UNK equipment             ',\n",
              " ' charles UNK has ashes spread over UNK , UNK iraq                 ',\n",
              " ' fast food customers less UNK than in commercial                   ',\n",
              " ' 10 people who made no difference in 2014                     ',\n",
              " ' 2016 in the economy                         ',\n",
              " ' 300,000 pounds of rat meat sold as chicken wings across america                  ',\n",
              " ' UNK fills bodies with UNK to save lives            ',\n",
              " ' dea UNK lil wayne to use up all drugs in mexico         ',\n",
              " ' study : congress literally doesn ’ t care what you think         ',\n",
              " ' grand jury indicted the man who filmed eric UNK ’ s killing        ',\n",
              " ' shocking report says even the smallest horse bite can be harmful to newborn babies      ',\n",
              " ' news : a huge spike : following the UNK UNK , ‘ golden showers ’ jumped from the most googled term to far and away the most googled term in america',\n",
              " ' sky UNK after being penetrated by prince ’ s spirit                     ',\n",
              " ' microsoft ’ s racist UNK returns with UNK twitter meltdown                     ',\n",
              " ' cancelled reality tv show still a key source of info for canadians on border agency , report finds             ',\n",
              " ' hackers access children ’ s names , photos in UNK breach                 ',\n",
              " ' UNK mission parents question school district ’ s purchase of UNK rifles                ',\n",
              " ' my life with the UNK ‘ fuck ’                    ',\n",
              " \" supreme court on gay marriage : UNK , who cares '          \",\n",
              " \" fan ca n't believe he left 11 seconds into UNK UNK fight        \",\n",
              " ' urban outfitters introducing clothing rental service                     ',\n",
              " ' tim cook just changed his twitter name to tim apple               ',\n",
              " ' man kills friend trying to scare away his UNK                ',\n",
              " ' ‘ UNK ’ updated to UNK man who is UNK being ordered to kill by the moon         ',\n",
              " ' ruth bader ginsburg suspended for next 10 UNK following supreme court UNK brawl             ',\n",
              " ' UNK finds UNK gene using ice bucket challenge funds                 ',\n",
              " ' un unveils design for floating city for 10,000 people                 ',\n",
              " ' sexist media keeps only referring to woman as ‘ bride of isis soldier ’             ',\n",
              " ' cbs to feature UNK fan to provide partially correct UNK of UNK ’ calls              ',\n",
              " ' congress actually does something                        ',\n",
              " ' men who robbed delivery person got chinese food , but no cash , UNK cops say                      ',\n",
              " ' colorado running out of UNK just one week after legalization                            ',\n",
              " \" we have our own version in the middle east , called the pan arabia UNK . it may not be at the same level , but it 's UNK content and brings a lot of UNK around our areas . check it out for some new flavor . ( UNK , sorry for UNK link )\",\n",
              " \" i 'll bet a lot of moms watching right now are thinking `` fuck you ''                                        \",\n",
              " ' amazon workers attempting UNK enter UNK hour wandering in UNK , UNK warehouse                                           ',\n",
              " ' deceased souls backed up at river UNK ferry crossing during UNK transit strike    ',\n",
              " ' the case for and against getting rid of the UNK                ',\n",
              " ' marathon training tips                       ',\n",
              " ' mysterious UNK skin disease continues to eat away at baby ’ s face weeks after being kissed by ted cruz ',\n",
              " ' mental health company offering UNK fans free therapy sessions                                   ',\n",
              " ' funeral meal leaves at least 9 dead , dozens sick in UNK                                ',\n",
              " ' maple tree wishes it was given a say in becoming memorial to man ’ s dead wife       ',\n",
              " ' rare UNK : because of the UNK calendar , 9 / 11 will fall on christmas eve this year     ',\n",
              " \" police called as playstation 4 tantrum leads to UNK ' son             \",\n",
              " ' fraternity brothers make note not to kill pledge whose family has lake house               ',\n",
              " \" how obama 's immigration plan would work                     \",\n",
              " ' marijuana used to get lobsters stoned before UNK to death                          ',\n",
              " ' papa john ’ s trying to stop papa john from taking over papa john ’ s                    ',\n",
              " ' study finds that monopoly players who are given large UNK still act UNK towards opponents and UNK they ’ ve won using UNK and strategy .          ',\n",
              " ' jay-z tried to have UNK ’ s name changed            ',\n",
              " ' UNK UNK during panicked 911 call ( with recording )           ',\n",
              " \" furry orgy breaks out at UNK ' premiere .            \",\n",
              " ' dedicated russell UNK UNK late after practice to miss 100 extra shots         ',\n",
              " ' subway agrees to measure sandwiches as part of settlement            ',\n",
              " ' car rolls up to UNK blasting google maps UNK            ',\n",
              " ' one person shows up to UNK event in iowa , remains UNK         ',\n",
              " ' the average college freshman reads at UNK grade level            ',\n",
              " ' tourists are getting hair transplant surgeries in turkey right now           ',\n",
              " ' UNK UNK cigarette on parade float , setting it on fire on the way to UNK christmas parade   ',\n",
              " ' washing machine loses man ’ s trust                                ',\n",
              " ' al man on mission to UNK UNK in all 50 states                            ',\n",
              " ' UNK removes sign above locker room door featuring UNK art UNK quote      ',\n",
              " ' martin shkreli faces UNK stay in prison system where inmates who funded hair theft are UNK UNK ',\n",
              " \" pillow that survived man 's UNK and turning stares frozen in horror at fallen UNK lying on ground\",\n",
              " ' ice agents UNK pregnant immigrant over mexican border to prevent birth on u.s. UNK    ',\n",
              " ' habitat for humanity investigated for working conditions after 92-year-old UNK collapses on site             ',\n",
              " ' humans gave UNK to UNK . now , they ’ re giving it back       ',\n",
              " \" trump UNK denies using word UNK ' to describe african immigrants          \",\n",
              " ' pope francis offers molested kids 10 % off at vatican city gift shop         ',\n",
              " ' editorial cartoon : ‘ sun burned ’               ',\n",
              " \" aides UNK drill from trump 's hands as he tries to remove obama listening device from skull     \",\n",
              " ' UNK police warn of alarming clown epidemic               ',\n",
              " \" studio developing UNK cat ' movie : “ we can build a big family comedy around this character ”          \",\n",
              " ' 8-year-old accidentally UNK second amendment rights                       ',\n",
              " ' UNK my ride : UNK proudly shows off a tank , turns out to just be a car           ',\n",
              " ' social media linked to increased UNK                       ',\n",
              " ' alabama forced to release thousands of sex offenders after inmates deny charges                 ',\n",
              " ' 2 men with 29 wives and UNK children between them sentenced to house arrest following UNK conviction       ',\n",
              " ' male substitute teacher with ponytail UNK in mystery                ',\n",
              " ' county finally accepts UNK man ’ s check with ‘ sexual UNK ’ written on memo line       ',\n",
              " ' scientist finally figures out why holes feel larger with your tongue than with your finger      ',\n",
              " ' pakistan erased an entire international new york times cover story .          ',\n",
              " ' bill clinton drew UNK and UNK on secret documents when he was president     ',\n",
              " ' a third of americans want to tax hipsters for ‘ being so annoying ’              ',\n",
              " ' UNK given nra membership for UNK UNK                     ',\n",
              " ' new honda commercial openly says your kids will die in a car crash if you buy a different brand              ',\n",
              " ' woman adopts second cat for first one to UNK while she at work                    ',\n",
              " ' coast guard terror suspect released after cell needed for UNK drug user                     ',\n",
              " ' parents finally cave and buy UNK son playstation 1                        ',\n",
              " ' new amazon service lets customers boost shipping speed with easy UNK charge to UNK delivery person                        ',\n",
              " ' olive oil in skinny bottle obviously better                                 ',\n",
              " ' UNK gas prices forcing more americans to drink less gas                              ',\n",
              " \" exclusive : jimmy UNK of 'the rent is too damn high ' receives UNK notice   \",\n",
              " ' vladimir putin begins second term as whatever he is         ',\n",
              " ' customer has a heart attack at the heart attack grill in vegas      ',\n",
              " ' lockdown of UNK UNK caused by UNK of bubble wrap        ',\n",
              " \" democrats reject barr 's offer to see UNK mueller report        \",\n",
              " ' black friday by the numbers                      ',\n",
              " ' income inequality emerges as key topic to avoid in 2014 elections            ',\n",
              " ' portrait next to coffin most likely the deceased               ',\n",
              " ' chris brown furious after rihanna ’ s home is broken into : he wishes he could protect her     ',\n",
              " ' denver ’ s flaming skull mayor announces plans to decriminalize magic mushrooms                               ',\n",
              " ' college basketball star UNK UNK tragic rape he committed                                  ',\n",
              " ' at the age of UNK , sir david UNK is taking a UNK UNK into dance music      ',\n",
              " ' UNK chan UNK UNK UNK solar panel efficiency by a massive 22 %          ',\n",
              " ' man races against time to take out trash bag with UNK UNK           ',\n",
              " ' woman knows to stay away from certain parts of own UNK at night               ',\n",
              " ' muhammad ali jr stopped again after UNK about first airport detention                 ',\n",
              " ' nate silver defends torture UNK used to make election UNK                  ',\n",
              " ' has prince william ever had a hot dog ? an investigation           ',\n",
              " ' nfl player works UNK fast-food job in UNK              ',\n",
              " ' nyc opens $ 500 million UNK subway station to catch UNK UNK      ',\n",
              " \" copy of 'the UNK letter ' ca n't believe the notes high schooler writing in UNK  \",\n",
              " ' theresa may : trump told me to sue the eu        ',\n",
              " ' owl forces her owner to watch movies that she hates                      ',\n",
              " ' world of warcraft classic UNK UNK are reporting vanilla wow features as bugs                   ',\n",
              " ' zoo hosts contest to name baby of pregnant gift shop worker                     ',\n",
              " ' nelson mandela becomes first politician to be missed                 ',\n",
              " ' businessman goes home for the holidays to network with family                ',\n",
              " \" gop introduces new `` mystery candidate '' with paper bag over head              \",\n",
              " \" pope francis rides into st. peter 's square on giant glowing UNK for easter mass           \",\n",
              " ' chinese leonardo dicaprio fans UNK lack of ‘ little golden man ’ for star           ',\n",
              " ' i refuse to UNK my son because it keeps growing back              ',\n",
              " ' man ’ s heart stops as speaker asks audience to turn to person next to them         ',\n",
              " ' world ’ s greatest soccer stars arrive in brazil for UNK coca-cola ad       ',\n",
              " ' court rules meryl streep unable to be tried by jury as she has no peers     ',\n",
              " \" rep. UNK 's murder of a UNK             \",\n",
              " ' trump denies existence of 2016 russia meeting UNK UNK                            ',\n",
              " ' red wine UNK report feeling sexy , but also just want to sleep                                        ',\n",
              " \" donald trump officially names obamacare replacement 'world 's greatest healthcare plan of 2017 ’                                       \",\n",
              " ' fingerprints on UNK trophy to be used in dozens of criminal UNK                                         ',\n",
              " ' thai soccer player still waiting for parents to pick him up                                          ',\n",
              " ' UNK UNK hold mass prayer against peace talks               ',\n",
              " ' a powerball UNK go fund me page has been created             ',\n",
              " ' ‘ i faked it all ’ : influencer UNK UNK faked entire trip to UNK        ',\n",
              " ' study : UNK percent of humans would rather be UNK bear                        ',\n",
              " ' ‘ i ’ m going to get you UNK ’ : UNK chases man who UNK her in cambridge ',\n",
              " ' former prom king now living anonymously among UNK          ',\n",
              " ' drama UNK UNK UNK with graphic video of her vagina        ',\n",
              " ' man in UNK loud UNK shirt explains google ’ s UNK new design language           ',\n",
              " ' check out adele singing “ UNK UNK , ” then watch some UNK guys sing UNK         ',\n",
              " ' pill to UNK : UNK drugs UNK into water may be making UNK UNK             ',\n",
              " \" UNK satellite says : `` UNK in UNK ''                  \",\n",
              " ' firefighters urge common sense after penis freed from toaster                  ',\n",
              " ' north korea open to UNK nuclear arms                 ',\n",
              " ' UNK trench : UNK UNK UNK finds plastic bag               ',\n",
              " \" by the time UNK harper 's UNK contract UNK , UNK will be a lonely , old , useless UNK     \",\n",
              " ' if first lady michelle obama could pick any other job , ‘ i would be beyonce ’        ',\n",
              " ' changing digital economy : the UNK has laid off 80 percent of its firefighters because it isn ’ t getting enough traffic on UNK             ',\n",
              " ' UNK UNK angels target businesses by posting UNK reviews                            ',\n",
              " ' court : movie theaters must UNK UNK patrons                             ',\n",
              " ' UNK million dead in UNK black friday weekend on record               ',\n",
              " ' jack UNK to give ethics talk at UNK school of business              ',\n",
              " ' this internet theory suggests all UNK fans live in the same universe             ',\n",
              " ' david UNK accidentally shoots self during trick                   ',\n",
              " ' UNK mouse UNK maze researchers spent months building                       ',\n",
              " ' building on the past : the supreme court has UNK president trump ’ s travel ban by citing the UNK legal UNK of the united states acting like an UNK UNK',\n",
              " ' lyft says it will make every ride carbon UNK                      ',\n",
              " ' doug baldwin sick of being UNK by statistics        ',\n",
              " ' woman has UNK UNK , nose , eyes , mouth      ',\n",
              " ' russian man recalls UNK days under UNK when no one could speak UNK or protest government',\n",
              " ' UNK probe badly damaged after smashing into end of universe              ',\n",
              " ' man approaches box of UNK UNK like snake discovering UNK UNK of bird eggs       ',\n",
              " ' death officially a motherfucker                 ',\n",
              " ' UNK fires UNK terrorist employee for failing to fill out UNK reports , execute UNK attacks     ',\n",
              " ' evangelical church strips away all the UNK and UNK of catholic UNK                     ',\n",
              " ' robbers get locked in new jersey cellphone store , crowd watches and UNK : video                  ',\n",
              " ' north korean UNK says kim jong-un won ’ t last                       ',\n",
              " \" UNK 's sexual UNK impatient for gay marriage slippery slope to kick in           \",\n",
              " ' my little pony : friendship is magic is racist !              ',\n",
              " ' UNK industry UNK as more moms making vaccines at home              ',\n",
              " ' only one person in the whole u.k. showed up to shia labeouf ’ s latest movie              ',\n",
              " ' UNK UNK . weather UNK for nyc .                      ',\n",
              " ' tim duncan : an nba legend rides into the UNK at a safe and UNK speed     ',\n",
              " ' nsa panel member recommends increased data collection            ',\n",
              " ' trump puts bannon on security council , dropping joint UNK         ',\n",
              " ' when good tv goes bad : the worst episodes of the best tv shows             ',\n",
              " \" will someone please UNK this UNK UNK of a cat ' ?                \",\n",
              " ' white house increases number of asylum UNK allowed to enter UNK refugee UNK               ',\n",
              " ' UNK mother UNK immunity through disease exposure                         ',\n",
              " ' hare UNK : ‘ hare UNK , hare UNK , UNK UNK , hare hare ’                ',\n",
              " ' sale ad for 2016 ford focus UNK promises free UNK UNK with purchase                   ',\n",
              " ' historians discover thomas UNK may have secretly UNK multiple other countries             ',\n",
              " \" jared kushner forced to follow along with UNK 's classified documents during meetings           \",\n",
              " ' viral chinese video game measures which players can clap UNK for president xi jinping          ',\n",
              " ' mom UNK into school office and UNK wrong child                       ',\n",
              " ' UNK running late will have to eat her babies on the go                    ',\n",
              " \" UNK browns coaches held UNK browns coaches party ' at nfl UNK combine                       \",\n",
              " \" teens UNK to town ' with restaurant comment cards | the onion               \",\n",
              " ' heavy UNK could affect fertility                  ',\n",
              " ' the events depicted in ‘ star wars ’ actually happened to me      ',\n",
              " ' entire arena football team fired during UNK meal          ',\n",
              " ' jeremy the UNK still lonely after potential UNK only have UNK for each other    ',\n",
              " ' UNK chinese kid cuts off own finger with kitchen knife        ',\n",
              " ' UNK radio station criticised for killing baby rabbit on air            ',\n",
              " ' florida republican party to hold voter registration at gun show one week after mass shootings       ',\n",
              " ' michael UNK jordan buys teen a new UNK after she bit through it during his shirtless black panther scene',\n",
              " ' north korea tests out new knife in smaller UNK of threats to u.s .                  ',\n",
              " ' chinese factory workers fear they may never be replaced with machines                     ',\n",
              " ' intelligence setback : the cia is in crisis mode after isis made its instagram private       ',\n",
              " ' swearing while you exercise could make you UNK : study            ',\n",
              " ' samsung warns users to watch what they say in front of smart tvs         ',\n",
              " ' most popular young adult fiction books                    ',\n",
              " ' UNK had a nuclear UNK with enough UNK uranium to start their own war            ',\n",
              " ' gaining UNK : marine UNK UNK is UNK in the polls after UNK to UNK france ’ s population to the 10 UNK people on earth',\n",
              " ' more women received UNK under UNK           ',\n",
              " ' city of baltimore targeting young UNK with new ‘ you get used to it ’ campaign        ',\n",
              " ' santorum : ‘ maybe it wasn ’ t ’ bad that UNK put limits on who could vote      ',\n",
              " ' bad news , gamers : nintendo revealed that in the next ‘ animal crossing ’ your character can ’ t find work and UNK home playing ‘ animal crossing ’ during their unemployment',\n",
              " ' confused zoo officials UNK celebrate after endangered panda gives birth to healthy northern white rhino                  ',\n",
              " ' kelly UNK makes racial UNK while slamming trump for racial UNK                      ',\n",
              " ' mark zuckerberg confirms facebook is working on UNK technology                    ',\n",
              " ' UNK duncan spends visit to local elementary school looking at ufo books in library                     ',\n",
              " ' busy schedule forces vladimir putin to move up election win a couple days early                     ',\n",
              " ' UNK UNK files lawsuit against parents for UNK russia investigation by giving birth to total UNK                   ',\n",
              " ' ‘ white privilege ’ essay contest sparks backlash                           ',\n",
              " ' thousands wait overnight at microsoft stores for second generation UNK          ',\n",
              " ' reality check : trip to UNK cost less than UNK stadium         ',\n",
              " \" autistic reporter , michael UNK , enchanted by prison 's UNK routine             \",\n",
              " ' this 19-year-old is paying her way through college by naming over UNK chinese babies      ',\n",
              " ' police use racist texts to mock the previous racist text scandal               ',\n",
              " ' how police are UNK their tactics                    ',\n",
              " ' 8-year-old follows UNK lawmaker around capitol until he drops welfare bill               ',\n",
              " ' man missing for 3 years found where last seen                 ',\n",
              " ' kim jong UNK announces plan to bring moon to north korea               ',\n",
              " ' war on string may be UNK , says cat general               ',\n",
              " ' lawrence UNK interviews empty chair                    ',\n",
              " ' cleveland hero charles UNK rewarded with burgers for life                ',\n",
              " ' who drummer UNK moon asked to perform at 2012 olympics ceremony             ',\n",
              " ' anti-vax mom changes her tune when all 7 of her children come down with UNK cough        ',\n",
              " ' sleeping man UNK by laptop , phone , UNK like egyptian UNK buried with all his UNK       ',\n",
              " \" detectives UNK UNK anthony 's ' i killed my daughter ' UNK on reddit              \",\n",
              " ' weather channel accused of UNK bias                    ',\n",
              " ' wtf ? UNK UNK body set for rebrand over use of UNK              ',\n",
              " ' scientists discover eating serves UNK other than UNK anxiety                 ',\n",
              " \" man thanks god he 's not sexually attracted to children                     \",\n",
              " ' stormy daniels ‘ 60 minutes ’ interview leads to spike in pornhub searches for anderson cooper               ',\n",
              " \" rob UNK ’ s wife gets out photo album to prove to him he 's met tom brady             \",\n",
              " ' man named kim adds ‘ mr. ’ to resume , lands job       ',\n",
              " ' UNK meet with top pentagon leaders to talk about peace         ']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "val_train_loop_incorrect(model, test_iterator)"
      ],
      "id": "6-azPje88iU0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie9VqRbg78Ty"
      },
      "source": [
        "### Part 6: LSTM Model [Extra-Credit, 4 points]"
      ],
      "id": "Ie9VqRbg78Ty"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXWSfPfBA4XU"
      },
      "source": [
        "#### Part 6.1 Define the RecurrentModel class\n",
        "Something that has been overlooked so far in this project is the sequential structure to language: a word typically only has a clear meaning because of its relationship to the words before and after it in the sequence, and the feed-forward network of Part 2 cannot model this type of data. A solution to this, is the use of [recurrent neural networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). These types of networks not only produce some output given some step from a sequence, but also update their internal state, hopefully \"remembering\" some information about the previous steps in the input sequence. Of course, they do have their own faults, but we'll cover this more thoroughly later in the semester. \n",
        "\n",
        "Your task for the extra credit portion of this assignment, is to implement such a model below using a LSTM. Instead of averaging the embeddings as with the FFN in Part 2, you'll instead feed all of these embeddings to a LSTM layer, get its final output, and use this to make your prediction for the class of the headline. "
      ],
      "id": "gXWSfPfBA4XU"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "YN8zvhLJ-MVJ"
      },
      "outputs": [],
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "    # Instantiate layers for your model-\n",
        "    # \n",
        "    # Your model architecture will be an optionally bidirectional LSTM,\n",
        "    # followed by a linear + sigmoid layer.\n",
        "    #\n",
        "    # You'll need 4 nn.Modules\n",
        "    # 1. An embeddings layer (see nn.Embedding)\n",
        "    # 2. A bidirectional LSTM (see nn.LSTM)\n",
        "    # 3. A Linear layer (see nn.Linear)\n",
        "    # 4. A sigmoid output (see nn.Sigmoid)\n",
        "    #\n",
        "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    # HINT: Think about what happens to the linear layer's hidden_dim size\n",
        "    #       if bidirectional is True or False.\n",
        "    # \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \\\n",
        "                 num_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE STARTS HERE (~4 lines of code) ##\n",
        "        self.embd = nn. Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn. LSTM(embedding_dim, hidden_dim, num_layers= num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        if bidirectional:\n",
        "          self.linear = nn. Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "          self.linear = nn. Linear(hidden_dim, 1)\n",
        "        self.sig = nn. Sigmoid()\n",
        "        \n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        \n",
        "    # Complete the forward pass of the model.\n",
        "    #\n",
        "    # Use the last timestep of the output of the LSTM as input\n",
        "    # to the linear layer. This will only require some indexing \n",
        "    # into the correct return from the LSTM layer. \n",
        "    # \n",
        "    # args:\n",
        "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    #     This is the same output that comes out of the collate_fn function you completed-\n",
        "    #\n",
        "    # returns:\n",
        "    # 1D FloatTensor of shape (BATCH_SIZE)\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE STARTS HERE (~4-5 lines of code) ##\n",
        "        em = self.embd(x)\n",
        "        # print(em.shape)\n",
        "        ls, _ = self.lstm(em)\n",
        "        # print(ls[0].shape)\n",
        "        li = self.linear(ls[:,-1, :])\n",
        "        # print(li.shape)\n",
        "        out = torch.flatten(self.sig(li))\n",
        "        # print(out.shape)\n",
        "\n",
        "        return out\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    "
      ],
      "id": "YN8zvhLJ-MVJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HprkOm-fAVyj"
      },
      "source": [
        "Now that the RecurrentModel is defined, we'll reinitialize our dataset iterators so they're back at the start. "
      ],
      "id": "HprkOm-fAVyj"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "6-uftfXEAqOi"
      },
      "outputs": [],
      "source": [
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "6-uftfXEAqOi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qROtRw3AtZy"
      },
      "source": [
        "#### Part 6.2 Initialize the LSTM classification model\n",
        "\n",
        "Next we need to initialize our new model, as well as define it's optimizer and loss function as we did for the FFN. Feel free to use the same optimizer you did above, or see how this model reacts to different optimizers/learning rates than the FFN.  "
      ],
      "id": "2qROtRw3AtZy"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "LNWcLJpsBRzg"
      },
      "outputs": [],
      "source": [
        "lstm_model = RecurrentModel(vocab_size    = len(train_vocab.keys()),\n",
        "                            embedding_dim = 300,\n",
        "                            hidden_dim    = 300,\n",
        "                            num_layers    = 1,\n",
        "                            bidirectional = True).to(device)"
      ],
      "id": "LNWcLJpsBRzg"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "OdTxe0bFBqnP"
      },
      "outputs": [],
      "source": [
        "lstm_criterion, lstm_optimizer = None, None\n",
        "### YOUR CODE STARTS HERE ###\n",
        "\n",
        "lstm_criterion = nn.MSELoss()\n",
        "lstm_optimizer = Adam(lstm_model.parameters())\n",
        "\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "id": "OdTxe0bFBqnP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFvV7H7OBzWl"
      },
      "source": [
        "#### Part 6.3 Training and Evaluation\n",
        "\n",
        "Because the only difference between this model and the FFN is the internal structure, we can use the same methods as above to evaluate and train it. You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "NFvV7H7OBzWl"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "SdkEpedxDopv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62661ec8-a9a9-42b2-fa6f-e507f3550b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 166.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Binary Macro F1: 0.270344189670795\n",
            "Accuracy: 0.3620833456516266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Pre-training to see what accuracy we can get with random parameters\n",
        "true, pred = val_loop(lstm_model, val_iterator)\n",
        "print()\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "SdkEpedxDopv"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "6p2dF9X4DyIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a1c35a-be11-44be-d9b6-26477e067008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:10<00:00, 116.17it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 202.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 189.75631734682247\n",
            "VAL F-1: 0.8138683341643669\n",
            "VAL ACC: 0.8262500166893005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 120.71it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 202.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 95.66249386500567\n",
            "VAL F-1: 0.8401942254798014\n",
            "VAL ACC: 0.8537499904632568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 120.60it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 205.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 53.88344313201378\n",
            "VAL F-1: 0.8345075529706607\n",
            "VAL ACC: 0.8429166674613953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:10<00:00, 119.55it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 203.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 30.48796252151078\n",
            "VAL F-1: 0.8519711992690606\n",
            "VAL ACC: 0.8637499809265137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 120.34it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 200.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 21.09183402778831\n",
            "VAL F-1: 0.8550092297365329\n",
            "VAL ACC: 0.8650000095367432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 120.78it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 193.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 16.165403183288504\n",
            "VAL F-1: 0.8490708139556777\n",
            "VAL ACC: 0.8608333468437195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Watch the model train!\n",
        "TOTAL_EPOCHS = 6\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(lstm_model, lstm_criterion, lstm_optimizer, train_iterator)\n",
        "    true, pred = val_loop(lstm_model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "6p2dF9X4DyIR"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "OR8Dl5DLEQwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03adf74-f399-4c1f-d299-f199a13cc1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 201.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST F-1: 0.850558856819468\n",
            "TEST ACC: 0.85916668176651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#See how your model does on the held out data\n",
        "true, pred = val_loop(lstm_model, test_iterator)\n",
        "print()\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "OR8Dl5DLEQwd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "source": [
        "### Part 7: Submit Your Homework\n",
        "This is the end. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/345683):\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p1_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended. \n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 4 & 6.3 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. Upload all 3 files to GradeScope.\n"
      ],
      "id": "mY8S9ZK9zuVs"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}